{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "==\n",
    "1: [Storing Tasks](#toc_1)\n",
    "1.1: [Save tweets from twitter to MongoDB](#toc_1.1)\n",
    "1.2: [Copy chunked tweets from HW 2 to MongoDB](#toc_1.2)\n",
    "2: [Retrieving and Analyzing Tasks](#toc_2)\n",
    "2.1: [Top Retweets](#toc_2.1)\n",
    "2.2: [Lexical Diversity](#toc_2.2)\n",
    "2.3: [Track unfollows](#toc_2.3)\n",
    "2.4: [Sentiment analysis](#toc_2.4)\n",
    "3: [Storing and Retrieving Task](#toc_3)\n",
    "3.1: [Backup & Restore](#toc_3.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_1'></a>",
    "1: Storing Tasks\n",
    "==\n",
    "<a name='toc_1.1'></a>",
    "1.1: Save tweets from twitter to MongoDB\n",
    "--\n",
    "Write a python program to automatically retrieve and store the JSON files (associated with the tweets that include #NBAFinals2015 hashtag and the tweets that include #Warriors hashtag) returned by the twitter REST api in a MongoDB database called db_restT. \n",
    "\n",
    "<a name='toc_1.2'></a>",
    "1.2: Copy chunked tweets from HW 2 to MongoDB\n",
    "--\n",
    "Write a python program to automatically retrieve and store the JSON files (associated with the tweets that include #NBAFinals2015 hashtag and the tweets that include #Warriors hashtag) returned by the twitter REST api in a MongoDB database called db_restT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are in files [1.1_acq.py](1.1_acq.py) and [1.2_s3tomongo.py](1.2_s3tomongo.py).\n",
    "\n",
    "The first uses much of the code from Assignment 2, with the addition of a `MongoDBSink` in `sinks.py` to store tweets to a specified database and collection.\n",
    "\n",
    "The second simply reads the files from S3 to a string, uses `json.loads` to read the contents into an array, and writes the tweets with `insert_many`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_2'></a>",
    "2: Retrieving and Analyzing Tasks\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "from  pandas import DataFrame\n",
    "from pprint import pprint\n",
    "import pymongo\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from  nltk.tokenize import word_tokenize\n",
    "import tweepy\n",
    "import os, os.path\n",
    "import datetime\n",
    "from credentials import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbclient = pymongo.MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "creds = Credentials(os.path.expanduser('~/.tweepy'))\n",
    "auth = tweepy.AppAuthHandler(creds.consumer_key, creds.consumer_secret)\n",
    "api = tweepy.API(auth_handler=auth,\n",
    "                  compression=True,\n",
    "                  retry_errors=set((104,)),\n",
    "                  retry_count=100,\n",
    "                  timeout=3600,\n",
    "                  wait_on_rate_limit=True,\n",
    "                  wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_2.1'></a>",
    "2.1: Top Retweets\n",
    "---\n",
    "Analyze the tweets stored in db_tweets by finding the top 30 retweets as well as their associated usernames (users authored them) and the locations of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index on retweeted_status.id will let us avoid both a collection scan and a sort when grouping retweets by id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'retweeted_status.id_1'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbclient.db_tweets.tweets.create_index('retweeted_status.id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a pymongo agg pipeline to get the top 30 retweets\n",
    "Since we need to count occurences, this requires an index scan.\n",
    "Instead of processing the results immediately, we save the result to another collection for subsequent, possibly repeated, analysis.\n",
    "The pattern I adopt here is that the code block that creates a table also drops/indexes it, for modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'count_1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbclient.db_tweets.drop_collection('top_retweets')\n",
    "cursor=dbclient.db_tweets.tweets.aggregate([\n",
    "        {'$match': {'retweeted_status.id' : { '$exists': True }}},\n",
    "        {'$group': {'_id': '$retweeted_status.id',\n",
    "                    'count': {'$sum': 1 },\n",
    "                    'user': {'$first': '$retweeted_status.user' },\n",
    "                    'text': {'$first': '$retweeted_status.text' }}},\n",
    "        {'$sort': {'count': -1 }},\n",
    "        {'$limit': 30 },\n",
    "        {'$out': 'top_retweets'}\n",
    "        ])\n",
    "dbclient.db_tweets.top_retweets.create_index('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the results of the aggregation pipeline by printing each result and adding each user ID to a set of most RTed users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: warriors from Oakland, CA retweeted 40727 times\n",
      "The wait is over ‚Äì Your Golden State #Warriors are #NBA CHAMPIONS!!! #GSW #NBAFinals http://t.co/3xxxrnJbJN\n",
      "#2: warriors from Oakland, CA retweeted 6363 times\n",
      "The #Warriors have won their first NBA Championship in 40 years after defeating the Cavs 105-97 in Game 6. http://t.co/kLe2TzALQu\n",
      "#3: StephenCurry30 from Worldwide retweeted 2513 times\n",
      "Dubnation!!!!!!!! Thank you for today. Will remember it for the rest of my life! #oakland #warriors‚Ä¶ https://t.co/zFE4GbDWD2\n",
      "#4: warriors from Oakland, CA retweeted 2383 times\n",
      "#Warriors close 1Q out on 8-0 run to take a 13-point lead into the startof 2Q. #StrengthInNumbers #GSW #NBAFinals http://t.co/0fKJzzwiHa\n",
      "#5: warriors from Oakland, CA retweeted 1675 times\n",
      "The #Warriors are set to face the Cavaliers in Game 6 of the #NBAFinals on Tuesday night in Cleveland.\n",
      "https://t.co/bu12LnXfGR\n",
      "#6: warriors from Oakland, CA retweeted 1434 times\n",
      ". @cavs answer back in 2Q &amp; what was a 13-point #Warriors advantage is cut to two at the break. #NBAFinals http://t.co/dkbbk2pumb\n",
      "#7: warriors from Oakland, CA retweeted 1373 times\n",
      "The #Warriors take the NBA title in Cleveland after beating the Cavs in Game 6 of the #NBAFinals.\n",
      "https://t.co/7I3eVUS4hl\n",
      "#8: SFGiantsFans from San Francisco, CA retweeted 1293 times\n",
      "BUSTER HUGS! #DubNation #Warriors #SFGiants #BusterHugs http://t.co/aGeSiZpF7H\n",
      "#9: warriors from Oakland, CA retweeted 1261 times\n",
      "Nice bit of decor in #Warriors Championship locker room. #StrengthInNumbers http://t.co/VyYjFzLM3E\n",
      "#10: warriors from Oakland, CA retweeted 1248 times\n",
      "The #Warriors are @NBA Champions! Celebrate with the locker room tee right here in this tweet! #NBAFinals http://t.co/t45zazC7X1\n",
      "#11: VineCommittee from Vine: VineCommittee retweeted 1145 times\n",
      "Steph curry drops stone cold stunner or the cavaliers üòÇüòÇüíÄ #Warriors https://t.co/WW1JyxzPdh\n",
      "#12: rickyrubio9 from El Masnou retweeted 1028 times\n",
      "Congratulations to the #Warriors What an incredible season they had. They deserve it. #NBAChamps\n",
      "#13: NBAMemes from  retweeted 1025 times\n",
      "Dwyane Wade &amp; Chris Bosh after watching LeBron James lose in the 2015 #NBAFinals.\n",
      "\n",
      "#Heat #Cavs #Warriors http://t.co/FtiT094VQE\n",
      "#14: warriors from Oakland, CA retweeted 872 times\n",
      "#Warriors retake the lead! #Splash (ABC) http://t.co/hQMTVjOjO0\n",
      "#15: warriors from Oakland, CA retweeted 784 times\n",
      "Started from the bottom, now we're here. The #Warriors are NBA Champions.\n",
      "\n",
      " ¬ª http://t.co/PTYinPcGt3 http://t.co/qVehKcGk4T\n",
      "#16: Whataburger from  retweeted 770 times\n",
      "Steph Curry is the NBA equivalent of a Honey Butter Chicken Biscuit #Warriors #NBAFinals\n",
      "#17: warriors from Oakland, CA retweeted 763 times\n",
      "The #Warriors got back to work in Cleveland this morning in preparation for Game 6 of the #NBAFinals tonight.\n",
      "https://t.co/zvEqcXX9qp\n",
      "#18: ciara from Making My Album.... retweeted 708 times\n",
      "Congratulations #Warriors!! Way To Win!!!!!!üëèüëèüëè\n",
      "#19: MLBMeme from  retweeted 690 times\n",
      "Thanks #Warriors for a great NBA Season, but..... http://t.co/cVaHYa3RNQ\n",
      "#20: WeLoveRobDyrdek from warning: 18+ content retweeted 670 times\n",
      "The wait is over ‚Äì Your Golden State #Warriors are #NBA CHAMPIONS!!! #GSW #NBAFinals http://t.co/deJ1O2vNSq\n",
      "#21: sal_castaneda from San Francisco, California retweeted 637 times\n",
      "Last night Oakland and San Francisco Police reported no major issues with celebrations for #Warriors NBA Championship.\n",
      "#22: ToniKroos from  retweeted 635 times\n",
      "Well deserved @warriors !!! The best team of the year. #NBAFinals2015\n",
      "#23: warriors from Oakland, CA retweeted 625 times\n",
      "#Warriors HQ greets @Andre with the MVP chants! https://t.co/IxerqwNXWg\n",
      "#24: WorIdStarComedy from  retweeted 582 times\n",
      "The wait is over ‚Äì Your Golden State #Warriors are #NBA CHAMPIONS!!! #GSW #NBAFinals http://t.co/IBbwJUCMC5\n",
      "#25: xoNaps from rŒπdgeland, –º—ï retweeted 571 times\n",
      "Warriors! #NBAFinals2015 #GSW\n",
      "#26: the_ironsheik from USA Bubba retweeted 559 times\n",
      "SETH CURRY HAVE THE SEX TONIGHT WITH ANYBODY HE WANT #NBAFinals2015\n",
      "#27: Oakland from Oakland, CA retweeted 548 times\n",
      "#Warriors blue and gold light Lake Merritt. Oakland loves our team! #dubnation #nbaplayoffs http://t.co/sJVOp7hD9Z\n",
      "#28: SFGiants from AT&T Park, San Francisco, CA retweeted 524 times\n",
      "Date night in the Bay Area for the Crawfords.  \n",
      "\n",
      "Go #Warriors #NBAFinals http://t.co/mXcSc44Rqy\n",
      "#29: BIGMONEYMIKE6 from bigmoneymike6@gmail.com retweeted 503 times\n",
      "I SHOULD BE THE MVP OF THE #NBAFinals2015 \n",
      "\n",
      "#TEAMBILLIONAIRE\n",
      "\n",
      "#BMM\n",
      "\n",
      "#PENNYSTOCKS \n",
      "\n",
      "#TEAMFAM\n",
      "\n",
      "JOIN OUR NETWORK: \n",
      "pennystockroyalty@gmail.com\n",
      "#30: JayKingzz_ from Philadelphia retweeted 500 times\n",
      "Congrats to the #GSW #NBAFinals2015\n"
     ]
    }
   ],
   "source": [
    "most_rt_users=set()\n",
    "cursor=dbclient.db_tweets.top_retweets.find({},sort=[('count', -1)])\n",
    "for (count, row) in enumerate(cursor):\n",
    "    user=row['user']\n",
    "    most_rt_users.add(user['id'])\n",
    "    print(u'#{0}: {1} from {2} retweeted {3} times\\n{4}'.format(\n",
    "            count+1,\n",
    "            user['screen_name'],\n",
    "            user['location'],\n",
    "            row['count'],\n",
    "            row['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_2.2'></a>",
    "2.2: Lexical Diversity\n",
    "---\n",
    "Compute the lexical diversity of the texts of the tweets for each of the users in db_restT and store the results back to Mongodb. To compute the lexical diversity of a user, you need to find all the tweets of a particular user (a user's tweets corpus), find the number of unique words in the user's tweets corpus, and divide that number by the total number of words in the user's tweets corpus.\n",
    "\n",
    "You need to create a collection with appropriate structure for storing the results of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first time we're looking at db_restT, so we start by indexing the tweet collection by user ID for subsequent aggregation on that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'user.id_1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbclient.db_restT.tweets.create_index('user.id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a table of db_restT unique users\n",
    "dbclient.db_restT.drop_collection('users')\n",
    "dbclient.db_restT.tweets.aggregate([\n",
    "        {'$group': {'_id': '$user.id',\n",
    "                    'name': {'$first': '$user.screen_name'}}},\n",
    "        {'$out': 'users'}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch each user's tweets and store them into `db_restT.user_tweets`, omitting RTs since they are not the user's text.\n",
    "Storing them permits comparison of different tokenizing methods and avoids the need to restart in case of failure.\n",
    "\n",
    "The `user_timeline` API per-call count limit is 200 statuses, and the call cannot retrieve more than 3200 tweets total. Limiting to 1000 tweets should be enough to be a representative sample, while eliminating 2/3 of the 15 minute rate limit delays.  The acquisition time can be further reduced by limiting the number of users (to 1000 of 6542) for whom we analyze statuses.\n",
    "\n",
    "(*Note*: I got impatient and interrupted the kernel after 870 users.)\n",
    "\n",
    "Some notes on implementation:\n",
    "* it is necessary to retrieve the user IDs from mongodb prior to the loop on `user_timeline` calls.  Otherwise the mongodb cursor in the outer loop will become invalue due to twitter rate limit waits.\n",
    "* `tweepy.API` has parameters to indicate which API status codes should be retried, but due to implementation details, 104 (connection reset by peer) errors get thrown.  These can be caught, and the `TweepError.response` object will be present, with member `status` set to 104. We can simply retry these, as the library will create a new connection.\n",
    "* Users with private timelines will fail with an \"authorization failed\" error, with no `response`.  We found their posts in the DB by hashtag, but are not permitted to enumerate their tweets. For these and all other exceptions, we can simply try going on to the next user.  This is a bit fragile, and will misbehave if, eg, there are `pymongo` exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 840\n",
      "Rate limit reached. Sleeping for: 829\n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "Rate limit reached. Sleeping for: 823\n",
      "Rate limit reached. Sleeping for: 779\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 824\n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "Rate limit reached. Sleeping for: 830\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "Rate limit reached. Sleeping for: 834\n",
      "Rate limit reached. Sleeping for: 812\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 820\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 826\n",
      "Rate limit reached. Sleeping for: 821\n",
      "Rate limit reached. Sleeping for: 814\n",
      "Rate limit reached. Sleeping for: 820\n",
      "Rate limit reached. Sleeping for: 816\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 814\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "Rate limit reached. Sleeping for: 814\n",
      "Rate limit reached. Sleeping for: 821\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 818\n",
      "Rate limit reached. Sleeping for: 820\n",
      "Rate limit reached. Sleeping for: 828\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 816\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Rate limit reached. Sleeping for: 812\n",
      "Rate limit reached. Sleeping for: 824\n",
      "Rate limit reached. Sleeping for: 821\n",
      "Rate limit reached. Sleeping for: 826\n",
      "{\"request\":\"\\/1.1\\/statuses\\/user_timeline.json\",\"error\":\"Not authorized.\"} \n",
      "Rate limit reached. Sleeping for: 828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5162fcdbf21b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mld_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mld_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-d8a31b109168>\u001b[0m in \u001b[0;36mld_fetch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtimeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                     dbclient.db_restT.user_tweets.insert_one(\n\u001b[0;32m     36\u001b[0m                         {'_id': tweet.id,\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/tweepy/cursor.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/tweepy/cursor.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m                                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                                         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Rate limit reached. Sleeping for:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# sleep for few extra sec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dbclient.db_restT.drop_collection('user_tweets')\n",
    "\n",
    "# chosen program limits\n",
    "tweet_limit=1000\n",
    "page_size=200 # API limit\n",
    "user_limit=1000\n",
    "\n",
    "userids=[]\n",
    "usernames=[]\n",
    "for row in dbclient.db_restT.users.find({}).limit(user_limit):\n",
    "    userids.append(row['_id'])\n",
    "    usernames.append(row['name'])\n",
    "for (userid,username) in zip(userids,usernames):\n",
    "    timeline=tweepy.Cursor(api.user_timeline,\n",
    "                           user_id=userid,\n",
    "                           count=page_size,\n",
    "                           trim_user=True,\n",
    "                           include_rts=False\n",
    "                          ).items(tweet_limit)\n",
    "    while True:\n",
    "        try: \n",
    "            for tweet in timeline:\n",
    "                dbclient.db_restT.user_tweets.insert_one(\n",
    "                    {'_id': tweet.id,\n",
    "                     'user': userid,\n",
    "                     'name': username,\n",
    "                     'text': tweet.text})\n",
    "            break\n",
    "        except tweepy.TweepError, e:\n",
    "            s=\"\"\n",
    "            if e.response and e.response.status:\n",
    "                s=e.response.status\n",
    "            print(e, s)\n",
    "            if s == 104:\n",
    "                continue\n",
    "            break\n",
    "        except Exception, e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze each user's tweets for lexical diversity.  I split on `\\W+` (one or more non-\"word\" character) to tokenize.  This is not always appropriate since we often wish to distinguish hashtags and mentions from other text.  However, since we are interested in words, this seems reasonable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbclient.db_restT.user_tweets.create_index('user')\n",
    "dbclient.db_restT.drop_collection('diversity')\n",
    "all_tweets=0\n",
    "users=dbclient.db_restT.users.find({})\n",
    "for user in users:\n",
    "    tweet_count=0\n",
    "    total_words=0\n",
    "    unique=set()\n",
    "    diversity=float('nan')\n",
    "    for (tweet_count,row) in enumerate(\n",
    "            dbclient.db_restT.user_tweets.find({'user': user['_id']})):\n",
    "        words=re.split(u'\\W+', row['text'])\n",
    "        unique.update(words)\n",
    "        total_words+=len(words)\n",
    "    if total_words > 0:\n",
    "        diversity=1.0*len(unique)/total_words\n",
    "    dbclient.db_restT.diversity.insert_one(\n",
    "        {'_id': user['_id'],\n",
    "         'name': user['name'],\n",
    "         'unique_words': len(unique),\n",
    "         'total_words': total_words,\n",
    "         'lexical_diversity': diversity,\n",
    "         'tweet_count': tweet_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6542 total users\n",
      "641016 total tweets\n",
      "870 users with tweets\n"
     ]
    }
   ],
   "source": [
    "print(dbclient.db_restT.users.count(), \"total users\")\n",
    "print(dbclient.db_restT.user_tweets.count(), \"total tweets\")\n",
    "print(len(list(\n",
    "            dbclient.db_restT.user_tweets.aggregate(\n",
    "                [{'$group': {'_id': '$user'}}]))), \"users with tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we plot the lexical diversity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f0015ec3550>]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG3dJREFUeJzt3XtwVPX9//HXZuMqhM2uCTe5aFCwwjpBSiK2WEHQdrQ6\nRDruTIEKqNOWjI7GS0UxU/xDLdgYooXq/GqLRTqQTmWHXme8JEi1M000oCwIItbLQMDclqAY2M35\n/eGwMd/shs3ZzW6Sz/Mxw0z27H523/smeeXkcz7nrMOyLEsAACNkZboAAED6EPoAYBBCHwAMQugD\ngEEIfQAwCKEPAAYh9AHAIIQ+BrVly5bp+uuvz2gNWVlZ2rx5c7fbf/rTnzJYERBfdqYLAJLhcDjk\ncDgyXUa3GhobG+XxeDJYDRAfoY9BzbIsDbSTykePHp3pEoC4mN7BkLJlyxZdccUVGjZsmCZNmqT7\n779fX375pSTprbfeksvlUiAQiD6+pqZGLpdLr7zySkLPX1NTo8LCQg0bNkzTp09XTU1Nj8d8c3pn\n8eLF+sEPftDjMTfccIN+8pOf2HmLQFIIfQx6Z6ZWNm7cqNLSUj344IPat2+f/vjHP+rVV1/Vz3/+\nc0nSd7/7Xa1evVp33HGHPv30U33++edasmSJ7rvvvoSOCxw+fFg33XSTiouL1dDQoIqKCt1zzz29\njlm2bJlee+01HTlyJLrtyJEjevXVV7V06dIk3jVgkwUMYkuXLrWuv/56y7Is66KLLrKef/75bvfv\n2LHDcjgcVltbm2VZltXZ2Wldd9111ve+9z3rhhtusGbNmmWFw+GEXmvVqlVWQUGBFYlEotv+9re/\nWQ6Hw9q8eXN02zdvRyIRa/z48dZTTz0Vvf+pp56yJk6caO8NA0liTx9DQlNTkz755BOVlZXJ7XZH\n/914441yOBw6ePCgpK//Kti0aZPeffdd7dy5U1u2bJHT6UzoNfbu3asrr7xSWVldPzazZ8/udUxW\nVpaWLFmiTZs2Rbdt2rRJixcvtvEugeRxIBdDQmdnpyTpmWee0bXXXtvj/vHjx0e/bmhoiM7zf/LJ\nJyooKEjoNRwOh62DxrfddpvWrl2r3bt3y7Isvffee9q6dWufnwdIBUIfQ8Lo0aM1ceJEvf/++7rj\njjviPq6xsVFLly7Vo48+qtbWVi1ZskS7d+/W+eeff9bXmDZtmjZt2qTOzs7o3v6bb76Z0LiZM2dG\nxxYVFemyyy5L/M0BKcT0Dga9M3vfjz/+uJ555hk98cQT2rNnj/bv369AIBA9kGtZlm677TZNmzZN\n5eXlWrt2rUaNGqXbb789oddZsWKFPv/8c/30pz/Vvn379Nprr2nVqlUJjb3tttu0efNmbdmyhQO4\nyKhe9/Q3bNighoYG5ebmqqKiQtLX85HvvPOOsrOzNWbMGJWWlmr48OGSpG3btqmmpkZZWVlavny5\npk+f3v/vAEb75slZS5Yskdvt1po1a/T4448rOztbF198sX70ox9JktauXat33nlHu3fvlsPh0Dnn\nnKMtW7Zo5syZ+u1vf6sVK1b0+lrjxo3TX//6V917772aMWOGLr30UlVVVWn+/PlnrXPRokV64IEH\n5HA49OMf/zj5Nw7Y1dtR3r1791qHDh2y7rvvvui23bt3R1cvvPTSS9ZLL71kWZZlffrpp9YDDzxg\nnT592jp69Kh11113dVvlEM+ePXvsHoQecuhFF3rRhV50oRdd7Pai1+mdqVOnKicnp9u2wsLC6Hzm\nlClT1NzcLEmqq6vT7NmzlZ2drdGjR2vs2LHRFRO9CQaDdn9fDTn0ogu96EIvutCLLnZ7kdSc/uuv\nv65vf/vbkqTW1lbl5+dH78vPz1dLS0syTw+klc/n67bc85v/SktLM10ekBK2V++8/PLLys7O1tVX\nXx33MQPhQlhAov71r3/p9OnTMe/Lzc1NczVA/7AV+rW1tWpoaFB5eXl0W15eXnSqR5Kam5uVl5fX\nY2wwGOz2Z4nf77dTwpBEL7pkohcTJ05M+2smgu+LLvSii9/vV3V1dfS2z+eTz+c767g+h/6uXbu0\nfft2rV69Wi6XK7q9qKhIVVVVuummm9TS0qLGxkZNnjy5x/hYhR0+fLivZQxJbrdb7e3ttsZmh1pk\nNR3tsd0xcozCnp6/fAe6ZHox1NCLLvSiy7hx42z9EnRYVvxTDNetW6d9+/bp+PHj8nq9uvXWWxUI\nBBQOhzVixAhJ0qWXXqo777xT0tdTPjU1NXI6nVq2bJmuuOKKhIog9L+WzDe088N9OvWrh3psd61c\no8glU5MtLe344e5CL7rQiy7jxo2zNa7XPf177723x7Z58+bFffzChQu1cOFCW4UAAPofZ+QCgEEI\nfQAwCKEPAAYh9AHAIIQ+ABiE0AcAgxD6AGAQQh8ADELoA4BBCH0AMAihDwAGIfQBwCCEPgAYhNAH\nAIPY/rhEpF+8D0qRJEc4nOZqAAxGhP4gYjUdjflBKZJ07j2/THM1AAYjpncAwCCEPgAYhNAHAIMQ\n+gBgEEIfAAxC6AOAQQh9ADAIoQ8ABiH0AcAghD4AGITQBwCDEPoAYJBeL7i2YcMGNTQ0KDc3VxUV\nFZKkEydOqLKyUk1NTRo1apTKysqUk5MjSdq2bZtqamqUlZWl5cuXa/r06f3/DgAACet1T//aa6/V\nI4880m1bIBBQYWGhqqqqdPnllysQCEiSPvvsM7311lt6+umn9cgjj+h3v/udOjs7+69yAECf9Rr6\nU6dOje7Fn1FfX685c+ZIkubOnau6ujpJUl1dnWbPnq3s7GyNHj1aY8eO1cGDB/upbACAHX2e0w+F\nQvJ6vZIkj8ejUCgkSWptbVV+fn70cfn5+WppaUlRmQCAVEjqQK7D4UjqfgBAevX5k7M8Ho/a2trk\n9XrV2toqj8cjScrLy1Nzc3P0cc3NzcrLy+sxPhgMKhgMRm/7/X653W47tQ85Lper1150OOP/d8X7\nBet0Zmv4IOzv2XphEnrRhV50V11dHf3a5/PJ5/OddUyfQ7+oqEi1tbUqKSnRjh07VFxcHN1eVVWl\nm266SS0tLWpsbNTkyZN7jI9VWHt7e1/LGJLcbnevvXBG4n8OrmVZMbdHIuFB2d+z9cIk9KILveji\ndrvl9/v7PK7X0F+3bp327dun48ePa8WKFfL7/SopKVFlZaVqamqiSzYlacKECfrOd76jsrIyOZ1O\n3XHHHUzvAMAA02vo33vvvTG3l5eXx9y+cOFCLVy4MPmqAAD9gjNyAcAghD4AGITQBwCDEPoAYBBC\nHwAMQugDgEEIfQAwCKEPAAYh9AHAIIQ+ABiE0AcAgxD6AGAQQh8ADELoA4BBCH0AMAihDwAGIfQB\nwCCEPgAYhNAHAIMQ+gBgEEIfAAxC6AOAQQh9ADAIoQ8ABiH0AcAghD4AGITQBwCDEPoAYBBCHwAM\nkm134LZt27Rz5045HA5deOGFKi0tVUdHhyorK9XU1KRRo0aprKxMOTk5qawXAJAEW3v6x44d02uv\nvaY1a9aooqJCnZ2devPNNxUIBFRYWKiqqipdfvnlCgQCqa4XAJAEW6E/fPhwOZ1OdXR0KBKJqKOj\nQ3l5eaqvr9ecOXMkSXPnzlVdXV1Ki0XfZbnOlfPDfTH/ZYdaMl0egDSzNb0zYsQI3XzzzSotLZXL\n5dL06dNVWFioUCgkr9crSfJ4PAqFQiktFn1nhVp1quqxmPe5Vq6RPHlprghAJtkK/cbGRv3973/X\n+vXrNXz4cD399NN64403uj3G4XDEHBsMBhUMBqO3/X6/3G63nTKGHJfL1WsvOpzx/7vi9Tvedkly\nOrM1fID2/my9MAm96EIvuquuro5+7fP55PP5zjrGVugfOnRI3/rWt6LNnzVrlg4cOCCv16u2tjZ5\nvV61trbK4/H0GBursPb2djtlDDlut7vXXjgj4bj3WZbVp+2SFImEB2zvz9YLk9CLLvSii9vtlt/v\n7/M4W3P648aN0wcffKBTp07Jsiy9++67mjBhgmbOnKna2lpJ0o4dO1RcXGzn6QEA/cTWnn5BQYGu\nueYarVy5Ug6HQ5MmTdJ1112nr776SpWVlaqpqYku2QQADBy21+kvWLBACxYs6LZtxIgRKi8vT7oo\nAED/4IxcADAIoQ8ABiH0AcAghD4AGITQBwCDEPoAYBBCHwAMQugDgEEIfQAwCKEPAAYh9AHAIIQ+\nABiE0AcAgxD6AGAQ25dWxuCX5TpX+nBfj+2OkWMU5rNzgSGJ0DdYvA9N5wPTgaGL6R0AMAh7+gNQ\ndqhFVtPRHtsd4fgfjA4AiSD0ByCr6ahO/eqhHtvPveeXGagGwFDC9A4AGITQBwCDEPoAYBBCHwAM\nQugDgEEIfQAwCKEPAAYh9AHAIIQ+ABjE9hm5X3zxhZ577jl99tlnkqTS0lJdcMEFqqysVFNTk0aN\nGqWysjLl5OSkrFgAQHJsh/4f/vAHzZgxQ/fff78ikYg6Ojr08ssvq7CwUAsWLFAgEFAgENDixYtT\nWS8AIAm2pne+/PJLvf/++5o3b54kyel0avjw4aqvr9ecOXMkSXPnzlVdXV3qKgUAJM3Wnv6xY8eU\nm5urDRs26OOPP9akSZO0bNkyhUIheb1eSZLH41EoFEppsQCA5NgK/Ugkoo8++ki33367Jk+erI0b\nNyoQCHR7jMPhiDk2GAwqGAxGb/v9frndbjtlDDkul0tut1sdztj/LfF62tt9dsY4ndkanuH/kzO9\nAL34JnrRXXV1dfRrn88nn8931jG2Qj8/P195eXmaPHmyJOmqq67Stm3b5PV61dbWJq/Xq9bWVnk8\nnh5jYxXW3t5up4whx+12q729Xc5I7OvmW5YVd2y8++yMiUTCGf8/OdML0Itvohdd3G63/H5/n8fZ\nmtP3er0aOXKkDh8+LEl69913NXHiRM2cOVO1tbWSpB07dqi4uNjO0wMA+ont1TvLly/Xs88+q3A4\nrDFjxqi0tFSdnZ2qrKxUTU1NdMkmAGDgsB36BQUFevLJJ3tsLy8vT6ogAED/4YxcADAIoQ8ABiH0\nAcAghD4AGITQBwCDEPoAYBBCHwAMQugDgEEIfQAwCKEPAAYh9AHAIIQ+ABiE0AcAgxD6AGAQQh8A\nDELoA4BBCH0AMAihDwAGsf1xiRi6slznSh/ui3mfY+QYhT15aa4IQKoQ+ujBCrXqVNVjMe9zrVwj\nEfrAoMX0DgAYhNAHAIMQ+gBgEOb0+yA71CKr6WiP7RzcBDBYEPp9YDUd1alfPdRjOwc3AQwWTO8A\ngEEIfQAwCKEPAAZJak6/s7NTK1euVF5enlauXKkTJ06osrJSTU1NGjVqlMrKypSTk5OqWgEASUpq\nT/8f//iHJkyYIIfDIUkKBAIqLCxUVVWVLr/8cgUCgZQUCQBIDduh39zcrIaGBs2bN0+WZUmS6uvr\nNWfOHEnS3LlzVVdXl5oqAQApYXt658UXX9SSJUt08uTJ6LZQKCSv1ytJ8ng8CoVCyVc4RMVa89/h\nzJYzEpYjHM5QVQCGOluh//bbbys3N1eTJk1SMBiM+ZgzUz7/VzAY7DbG7/fL7XbbKSPtOpyx2+V0\nZmt4H99Dx/8+0MkYa/4l6bx7V8fcHq+nvd2X6jF23qsdLpdr0Hxf9Dd60YVedFddXR392ufzyefz\nnXWMrdDfv3+/3n77bTU0NOj06dM6efKknn32WXk8HrW1tcnr9aq1tVUej6fH2FiFtbe32ykj7ZyR\n2HvgkUi4z+8h3nNJik6XJbo9nWPsvFc73G73oPm+6G/0ogu96OJ2u+X3+/s8zlboL1q0SIsWLZIk\n7d27V9u3b9fdd9+tl156SbW1tSopKdGOHTtUXFxs5+kBAP0kJev0z0wHlJSU6L333tM999yjPXv2\nqKSkJBVPDwBIkaSvvTNt2jRNmzZNkjRixAiVl5cnXRQAoH9wRi4AGITQBwCDEPoAYBBCHwAMQugD\ngEEIfQAwCKEPAAYh9AHAIIQ+ABiE0AcAgyR9GQZAiv35AGc4Ro5R2JOX5ooAxELoIyWspqM6Fefz\nAVwr10iEPjAgEProkyzXudKH+3ps59O+gMGB0EefWKFWnap6rMf2c+/5ZQaqAdBXHMgFAIMQ+gBg\nEEIfAAxC6AOAQQh9ADAIoQ8ABiH0AcAgrNNHv4t7QheXZwDSjtBHv4t3QheXZwDSj+kdADAIoQ8A\nBiH0AcAghD4AGITQBwCD2Fq909TUpPXr1ysUCsnhcGj+/Pm68cYbdeLECVVWVqqpqUmjRo1SWVmZ\ncnJyUl0zAMAmW6GfnZ2tpUuXqqCgQF999ZUeeughFRYWqra2VoWFhVqwYIECgYACgYAWL16c6poB\nADbZmt7xer0qKCiQJJ133nkaP368WlpaVF9frzlz5kiS5s6dq7q6upQVCgBIXtJz+seOHdP//vc/\nTZkyRaFQSF6vV5Lk8XgUCoWSLhAAkDpJhf5XX32liooKLVu2TMOGDet2n8PhSKowAEDq2b4MQzgc\nVkVFha655hpdeeWVkr7eu29ra5PX61Vra6s8Hk+PccFgUMFgMHrb7/fL7XbbLSOtOpyx2+V0Zmt4\nnPdwuvGwOj9v7LHdikTivk68X5i9/SIdjGN665vL5Ro03xf9jV50oRfdVVdXR7/2+Xzy+XxnHWMr\n9C3L0nPPPafx48frhz/8YXR7UVGRamtrVVJSoh07dqi4uLjH2FiFtbe32ykj7ZyRcMztkUg47ntw\nNn6mU796qMf23j5I3LKsPm0frGN665vb7R403xf9jV50oRdd3G63/H5/n8fZCv39+/dr586duvDC\nC/WLX/xCkrRo0SKVlJSosrJSNTU10SWbAICBw1boX3bZZdq6dWvM+8rLy5MqCADQfzgjFwAMwvX0\nkTHxPlxFkk6PnSDlcMAOSDVCHxkT78NVJMm5qoLQB/oBoZ8Cve2xOsKxV/wAQCYQ+inQ2x5rb0sz\nASDdOJALAAYh9AHAIEzvYFDJDrXIajoa8z7HyDEKe/LSXBEwuBD6GFSspqMxL2shSa6VayRCH+gV\n0zsAYBD29DEwnXOOnDGWwbIElikuJIfQx4BktbXo1LrVPbazBJYpLiSH6R0AMAihDwAGIfQBwCCE\nPgAYhAO5GDLiXfiutxUt8VbCpGsVDCtxkG6EPoaMeBe+621FS7yVMOlaBcNKHKQb0zsAYBBCHwAM\nwvQOEEOvH4xjY6497rGDXs4wPlNDhzNbzkg4oTHA2RD6QAy9fTCOnbn2eHP3vZ1hHK8GzkpGMpje\nAQCDEPoAYBCmd/6PXtdNM5c6KPHB9ZwPgC7Ghn7cH4JwWKd+vSrmGOZSB6dUf3C9nZPAMo3zAVIv\n0yf22WVs6Ns5sAZI9k4Cw9CT6RP77BrSoc9UDUwT96+QBJaG9hgzgC9fYcdgneKKW/e4cfaeL8l6\neti1a5c2btyozs5OzZs3TyUlJal+iYT19icte/QYiuws8xyMl6+wY7BOccWt+3v1tp4vpaHf2dmp\nF154QeXl5crLy9PDDz+soqIiTZgwIZUv04OdE1+AVOOAsT0DeQ/czmxBqk/sS7WUhv7Bgwc1duxY\njR49WpI0e/Zs1dfXpyT0HZGIHJ2dPe/IYn4eA0OqDxibYiDvgduZLUj1iX2pltLQb2lpUX5+fvR2\nXl6eDh48mJLndn60X6e3/L+e278zTyqYkpLXANAl1X+5JHMpijPOXJIi1XvMdo6FDFaD5kCuw5sn\n5+zremzPKpgiK9ZfAACSkuq/XFJ5KYpU7zGbdMkLh2VZVqqe7MCBA/rzn/+sVau+Xue+bds2ORyO\nbgdzg8GggsFg9Lbf70/VywOAUaqrq6Nf+3w++Xy+sw+yUigcDlt33XWXdfToUev06dPWAw88YH36\n6ae9jtm6dWsqSxjU6EUXetGFXnShF13s9iKl0ztOp1O33367Hn/88eiSzf5euQMASFzK5/RnzJih\nGTNmpPppAQApkPGrbCY0B2UIetGFXnShF13oRRe7vUjpgVwAwMCW8T19AED6EPoAYJC0nZyVyIXY\nfv/732vXrl0699xzVVpaqkmTJqWrvLQ6Wy927typ7du3y7IsDRs2THfeeacuuuiiDFXbvxK9QN/B\ngwf16KOPqqysTLNmzUpzlemRSC+CwaBefPFFRSIRud1urV69Ov2FpsHZenH8+HE9++yzamtrU2dn\np26++WbNnTs3M8X2ow0bNqihoUG5ubmqqKiI+Zg+52Yq143GE4lEzrp+/+2337aeeOIJy7Is68CB\nA9YjjzySjtLSLpFe7N+/3/riiy8sy7KshoYGo3tx5nGrV6+2nnzySes///lPBirtf4n04sSJE1ZZ\nWZnV1NRkWZZlhUKhTJTa7xLpxdatW63NmzdblvV1H5YvX26Fw+FMlNuv9u7dax06dMi67777Yt5v\nJzfTMr3zzQuxZWdnRy/E9k319fWaM2eOJGnKlCn64osv1NbWlo7y0iqRXlx66aUaPny4JGny5Mlq\nbm7ORKn9LpFeSNI///lPXXXVVcrNzc1AlemRSC/+/e9/a9asWdHrWw3VfiTSi/PPP19ffvmlJOnk\nyZNyu91yOp2ZKLdfTZ06VTk5OXHvt5ObaQn9WBdia2lp6fUx+fn5PR4zFCTSi296/fXXh+x5D4l+\nX9TX1+v73/++JMnhcKS1xnRJpBdHjhzRiRMn9Nhjj2nlypV644030l1mWiTSi/nz5+uzzz7Tz372\nMz344INatmxZmqscGOzk5oA6kGuxerSbPXv2qKamRosXL850KRmzceNGLVq0SA6HQ5ZlGf09EolE\n9NFHH+nhhx/WqlWr9Je//EVHjhzJdFkZsW3bNhUUFOj555/X2rVr9cILL+jkyZOZLisj+vozkZYD\nuXl5ed2mKJqbm5WXl9fnxwwFib7Pjz/+WM8//7xWrVqlESNGpLPEtEmkF4cOHdK6deskSe3t7dq1\na5eys7NVVFSU1lr7WyK9yM/Pl9vtlsvlksvl0tSpU/Xxxx/rggsuSHe5/SqRXhw4cEC33HKLJEWn\ngg4fPqxLLrkkrbVmmp3cTMue/iWXXKLGxkYdO3ZM4XBYb731Vo8f2qKiouifqwcOHFBOTo68Xm86\nykurRHrR1NSkX//617r77rs1duzYDFXa/xLpxW9+8xutX79e69ev11VXXaU777xzyAW+lFgviouL\ntX//fnV2dqqjo0MffPDBkLy2VSK9GDdunN577z1JUltbmw4fPqwxY8ZkotyMspObaTsjt6GhodsS\nrFtuuUWvvPKKJOn666+XJL3wwgvatWuXzjvvPK1YsUIXX3xxOkpLu7P14rnnntN///tfjRw5UtLX\nF7J78sknM1lyv0nk++KMDRs2aObMmUN2yWYivdi+fbtqa2vlcDg0f/583XjjjZksud+crRfHjx/X\nhg0b1NzcrM7OTt1yyy26+uqrM1x16q1bt0779u3T8ePH5fV6deuttyoSiUiyn5tchgEADDKgDuQC\nAPoXoQ8ABiH0AcAghD4AGITQBwCDEPoAYBBCHwAMQugDgEH+P9aLmPEiEF4AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0015d8ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cursor=dbclient.db_restT.diversity.aggregate([\n",
    "    {'$project': {\n",
    "            '_id': 0,\n",
    "            'name': '$name',\n",
    "            'lex_div': '$lexical_diversity'}}])\n",
    "lex_div=DataFrame(list(cursor))\n",
    "lex_div.hist('lex_div', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_2.3'></a>",
    "2.3: Track unfollows\n",
    "--\n",
    "Write a python program to create a db called db_followers that stores all the followers for all the users that you find in task 2.1. Then, write a program to find the un-followed friends after a week for the top 10 users( users that have the highest number of followers in task 2.1) since the time that you extracted the tweets. In other words, you need to look for the people following the top 10 users at time X (the time that you extracted the tweets) and then look at the people following the same top 10 users at a later time Y (one-week after X) to see who stopped following the top 10 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make a db/table of top RT'ed users in which to store follower stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'initial_count_1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbclient.drop_database('db_followers')\n",
    "rows=[]\n",
    "for row in dbclient.db_tweets.top_retweets.aggregate([\n",
    "        {'$group': {\n",
    "                '_id': '$user.id',\n",
    "                'name': {'$first': '$user.screen_name'},\n",
    "                'initial_count': {'$first': '$user.followers_count'}}}]):\n",
    "    rows.append(dict(row))\n",
    "dbclient.db_followers.top_rt_followers.insert_many(rows)\n",
    "dbclient.db_followers.top_rt_followers.create_index('initial_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fetch the list of followers for the top 10 by followers.\n",
    "The API rate limit is 15 calls per hour, and each call can return 5000 followers.  While this may compromise the efficacy of the assignment there are over 19 million followers for these users, I will only fetch the first 10k followers to avoid spending 11 days fetching all the followers (particularly since they would have changed in that time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make this a function to be able to call it at start and end\n",
    "def get_followers(tag):\n",
    "    follower_limit=10000\n",
    "    users=[]\n",
    "    for row in dbclient.db_followers.top_rt_followers.find(\n",
    "        {},sort=[('initial_count', -1)]).limit(10):\n",
    "        users.append(row['_id'])\n",
    "    for (num,user) in enumerate(users):\n",
    "        print(\"Processing user\", num)\n",
    "        cursor=tweepy.Cursor(\n",
    "                        api.followers_ids,\n",
    "                        count=5000,\n",
    "                        user_id=user).items(follower_limit)\n",
    "        while True:\n",
    "            try: \n",
    "                followers=[]\n",
    "                for follower in cursor:\n",
    "                    followers.append(follower)\n",
    "                dbclient.db_followers.top_rt_followers.update_one(\n",
    "                    {'_id': user},\n",
    "                    {'$set': { tag+'_followers': followers,\n",
    "                               tag+'_date': datetime.datetime.now()}},\n",
    "                    upsert=True)\n",
    "                break\n",
    "            except tweepy.TweepError, e:\n",
    "                s=\"\"\n",
    "                if e.response and e.response.status:\n",
    "                    s=e.response.status\n",
    "                print(e, s)\n",
    "                if s == 104:\n",
    "                    continue\n",
    "                break\n",
    "            except Exception, e:\n",
    "                print(e)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user 0\n",
      "Rate limit reached. Sleeping for: 690\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Processing user 1\n",
      "Processing user 2\n",
      "Processing user 3\n",
      "Processing user 4\n",
      "Processing user 5\n",
      "Processing user 6\n",
      "Processing user 7\n",
      "Processing user 8\n",
      "Rate limit reached. Sleeping for: 897\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Processing user 9\n"
     ]
    }
   ],
   "source": [
    "get_followers('initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a week, run the second phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user 0\n",
      "Processing user 1\n",
      "Processing user 2\n",
      "Processing user 3\n",
      "Processing user 4\n",
      "Processing user 5\n",
      "Processing user 6\n",
      "Processing user 7\n",
      "Rate limit reached. Sleeping for: 897\n",
      "Failed to send request: ('Connection aborted.', error(104, 'Connection reset by peer')) \n",
      "Processing user 8\n",
      "Processing user 9\n"
     ]
    }
   ],
   "source": [
    "get_followers('final')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And report the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciara:\n",
      "\tlost 0\tgained 0\tdelta 1902\n",
      "\tin 10 days, 2:21:58.343000\n",
      "ToniKroos:\n",
      "\tlost 0\tgained 0\tdelta 1496\n",
      "\tin 10 days, 2:21:58.420000\n",
      "StephenCurry30:\n",
      "\tlost 0\tgained 0\tdelta 986\n",
      "\tin 10 days, 2:21:58.441000\n",
      "BIGMONEYMIKE6:\n",
      "\tlost 0\tgained 0\tdelta -375\n",
      "\tin 10 days, 2:21:58.459000\n",
      "rickyrubio9:\n",
      "\tlost 0\tgained 0\tdelta -4199\n",
      "\tin 10 days, 2:21:58.517000\n",
      "warriors:\n",
      "\tlost 0\tgained 0\tdelta -1731\n",
      "\tin 10 days, 2:21:58.536000\n",
      "WeLoveRobDyrdek:\n",
      "\tlost 0\tgained 0\tdelta -4976\n",
      "\tin 10 days, 2:21:58.844000\n",
      "SFGiants:\n",
      "\tlost 0\tgained 0\tdelta 1377\n",
      "\tin 10 days, 2:21:58.914000\n",
      "the_ironsheik:\n",
      "\tlost 0\tgained 0\tdelta -4895\n",
      "\tin 10 days, 2:21:58.932000\n",
      "Whataburger:\n",
      "\tlost 0\tgained 0\tdelta 778\n",
      "\tin 10 days, 2:21:58.952000\n"
     ]
    }
   ],
   "source": [
    "for row in dbclient.db_followers.top_rt_followers.find(\n",
    "        {},sort=[('initial_count', -1)]).limit(10):\n",
    "    init=set(row['initial_followers'])\n",
    "    final=set(row['final_followers'])\n",
    "    print(\"{0}:\\n\\tlost {1}\\tgained {2}\\tdelta {3}\\n\\tin {4}\".format( \n",
    "          row['name'],\n",
    "          len(init-final), len(final-init),\n",
    "          row['final_count']-row['initial_count'],\n",
    "          row['final_date']-row['initial_date'],\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite changes to the follower counts, there were no IDs in the final set not in the initial one.  As I had feared, the sample size of 10000 followers was not enough to capture the changed follower IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name='toc_2.4'></a>",
    "2.4: Sentiment analysis\n",
    "--\n",
    "(Bonus task) Write a python program and use NLTK to analyze the top 30 retweets of task 2.1 as positive or negative (sentiment analysis). This is the bonus part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetched a corpus of classified tweets from\n",
    "# http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
    "# -- Note: Omitted from git push due to size.\n",
    "# Incorporating feature ideas described in Kiritchenko et al, Sentiment Analysis of Short Informal Texts\n",
    "# stemmed tokens, POS-tagged tokens, final hashtags,n-grams\n",
    "# As classifier use random forest from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='toc_3'></a>",
    "3: Storing and Retrieving Task\n",
    "===\n",
    "<a name='toc_3.1'></a>",
    "3.1: Backup & Restore\n",
    "---\n",
    "Write a python program to create and store the backups of both db_tweets and db_restT to S3. It also should have a capability of loading the backups if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backup a database, call `mongodump` to dump the database BSON files, compress them, and copy the archive to a user-provided S3 bucket.\n",
    "\n",
    "For this assignment, I am using s3 bucket `nkrishna-mids205-hw3`.  I think I forgot to grant ListBucket privs last time, so I have added them to this bucket as well as GetObject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from boto.s3.key import Key\n",
    "\n",
    "def dump_db(s3bucket, dbname):\n",
    "    dumpdir='dump/'+dbname\n",
    "    subprocess.call(['rm','-rf', dumpdir])\n",
    "    print(\"Dumping database\", dbname)\n",
    "    subprocess.call(['mongodump', '--db', dbname])\n",
    "    zipfile=dbname+'.zip'\n",
    "    print(\"Compressing database dump\")\n",
    "    subprocess.call(['zip', '-r', zipfile, dumpdir])\n",
    "    key=Key(bucket, zipfile)\n",
    "    print('Uploading database dump to s3://{0}/{1}'.format(\n",
    "        bucket.name, zipfile))\n",
    "    key.set_contents_from_filename(zipfile)\n",
    "    subprocess.call(['rm', '-rf', dumpdir, zipfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping database db_tweets\n",
      "Compressing database dump\n",
      "Uploading database dump to s3://nkrishna-mids205-hw3/db_tweets.zip\n",
      "Dumping database db_restT\n",
      "Compressing database dump\n",
      "Uploading database dump to s3://nkrishna-mids205-hw3/db_restT.zip\n"
     ]
    }
   ],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "s3creds=Credentials(os.path.expanduser('~/.aws/credentials'))\n",
    "conn=S3Connection(s3creds.default_aws_access_key_id,\n",
    "    s3creds.default_aws_secret_access_key)\n",
    "bucket_name='nkrishna-mids205-hw3'\n",
    "bucket=conn.lookup(bucket_name)\n",
    "if not bucket:\n",
    "    bucket=conn.create_bucket(bucket_name)\n",
    "dump_db(bucket, 'db_tweets')\n",
    "dump_db(bucket, 'db_restT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring will simply invert the steps: fetch the zipped BSON dumps, extract them, and call `mongorestore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restore_db(s3bucket, dbname):\n",
    "    dumpdir='dump/'+dbname\n",
    "    subprocess.call(['rm','-rf', dumpdir])\n",
    "    zipfile=dbname+'.zip'\n",
    "    key=Key(bucket, zipfile)\n",
    "    print('Downloading database dump from s3://{0}/{1}'.format(\n",
    "        bucket.name, zipfile))\n",
    "    key.get_contents_to_filename(zipfile)\n",
    "    print(\"Decompressing database dump\")\n",
    "    subprocess.call(['unzip', zipfile])\n",
    "    print(\"Restoring database\", dbname)\n",
    "    subprocess.call(['mongorestore', dumpdir])\n",
    "    subprocess.call(['rm', '-rf', dumpdir, zipfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading database dump from s3://nkrishna-mids205-hw3/db_tweets.zip\n",
      "Decompressing database dump\n",
      "Restoring database db_tweets\n",
      "Downloading database dump from s3://nkrishna-mids205-hw3/db_restT.zip\n",
      "Decompressing database dump\n",
      "Restoring database db_restT\n"
     ]
    }
   ],
   "source": [
    "dbclient.drop_database('db_tweets')\n",
    "restore_db(bucket, 'db_tweets')\n",
    "dbclient.drop_database('db_restT')\n",
    "restore_db(bucket, 'db_restT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the older sanity check still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6542 total users\n",
      "641016 total tweets\n",
      "870 users with tweets\n"
     ]
    }
   ],
   "source": [
    "print(dbclient.db_restT.users.count(), \"total users\")\n",
    "print(dbclient.db_restT.user_tweets.count(), \"total tweets\")\n",
    "print(len(list(\n",
    "            dbclient.db_restT.user_tweets.aggregate(\n",
    "                [{'$group': {'_id': '$user'}}]))), \"users with tweets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
